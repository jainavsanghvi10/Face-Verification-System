{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Face Verification using an AutoEncoder","metadata":{}},{"cell_type":"code","source":"import numpy as np ## for numerical calculations of arrays\nimport pandas as pd ## for reading csv file and wroking with dataframe operations\nfrom PIL import Image ## for image processing and output\nimport cv2 ## for image processing\nimport numpy\nimport os ## for reading images from image folder\nimport matplotlib.pyplot as plt\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-12T13:17:10.867487Z","iopub.execute_input":"2023-09-12T13:17:10.867923Z","iopub.status.idle":"2023-09-12T13:17:10.874095Z","shell.execute_reply.started":"2023-09-12T13:17:10.867875Z","shell.execute_reply":"2023-09-12T13:17:10.872876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras.backend import epsilon\nfrom tensorflow.keras.layers import InputLayer,Input,Dense, Conv2D, MaxPooling2D, UpSampling2D,Conv2DTranspose, Flatten, Reshape\nfrom tensorflow.keras.models import Model,Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import *\nrandom.seed(42)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:10.880928Z","iopub.execute_input":"2023-09-12T13:17:10.881702Z","iopub.status.idle":"2023-09-12T13:17:16.551570Z","shell.execute_reply.started":"2023-09-12T13:17:10.881668Z","shell.execute_reply":"2023-09-12T13:17:16.550593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Face detection and cropping","metadata":{}},{"cell_type":"code","source":"# Load OpenCV's pre-trained face detection cascade classifier\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:16.553374Z","iopub.execute_input":"2023-09-12T13:17:16.554110Z","iopub.status.idle":"2023-09-12T13:17:16.591131Z","shell.execute_reply.started":"2023-09-12T13:17:16.554075Z","shell.execute_reply":"2023-09-12T13:17:16.590021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the Dataset\nWe're reading the folders and splitting them into train and test set for training purposes.","metadata":{}},{"cell_type":"code","source":"# Set paths\ndataset_folder = '/kaggle/input/iiitb-faces/IIITB-FACES'\ntest_filepaths = [] # Contains the absolute paths of test images\ntrain_filepaths = []# Contains the absolute paths of train images\n\n# Loop through each person's folder\nfor person_folder in os.listdir(dataset_folder):\n    person_path = os.path.join(dataset_folder, person_folder)\n    person_images = [os.path.join(person_path, image_file) for image_file in os.listdir(person_path)]\n\n    random.shuffle(person_images)\n    # Calculate split point based on 80-20 ratio\n    split_index = int(0.8 * len(person_images))\n    \n    # Split images into train and test\n    train_filepaths.append(person_images[:split_index])\n    test_filepaths.append(person_images[split_index:])\n\nprint(\"Total images:\", len(train_filepaths)+len(test_filepaths))\nprint(\"Total train images:\", len(train_filepaths))\nprint(\"Total test images:\", len(test_filepaths))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:16.592345Z","iopub.execute_input":"2023-09-12T13:17:16.592655Z","iopub.status.idle":"2023-09-12T13:17:17.091391Z","shell.execute_reply.started":"2023-09-12T13:17:16.592628Z","shell.execute_reply":"2023-09-12T13:17:17.090257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_images = len(train_filepaths)+len(test_filepaths)\ntotal_train_images = len(train_filepaths)\ntotal_test_images = len(test_filepaths)\ntotal_classes = 49","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:17.093277Z","iopub.execute_input":"2023-09-12T13:17:17.093698Z","iopub.status.idle":"2023-09-12T13:17:17.099338Z","shell.execute_reply.started":"2023-09-12T13:17:17.093658Z","shell.execute_reply":"2023-09-12T13:17:17.098238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformed_train_filepaths = []\nfor filename in train_filepaths:\n    temp = []\n    for imagename in filename:\n        img = cv2.imread(imagename)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n\n        faces = face_cascade.detectMultiScale(img, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50))\n        face_image = img\n        if(len(faces) > 0):\n            xf, yf, wf, hf = faces[0]\n            face_image = img[yf:yf+hf, xf:xf+wf]\n        face_image = cv2.resize(face_image, (512, 512))\n        \n\n        temp.append(face_image)\n    transformed_train_filepaths.append(temp)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:17.103901Z","iopub.execute_input":"2023-09-12T13:17:17.104218Z","iopub.status.idle":"2023-09-12T13:17:53.007288Z","shell.execute_reply.started":"2023-09-12T13:17:17.104191Z","shell.execute_reply":"2023-09-12T13:17:53.006350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum = 0\nlis = []\nfrequency = {}\nfor i in range(0, len(transformed_train_filepaths)):\n    sum += ((len(transformed_train_filepaths[i]))*(len(transformed_train_filepaths[i]) - 1))/2\n    lis.append(len(transformed_train_filepaths[i]))\n\nfor item in lis:\n   # checking the element in dictionary\n   if item in frequency:\n      # incrementing the count\n      frequency[item] += 1\n   else:\n      # initializing the count\n      frequency[item] = 1\ntot_val = 0\nfor key in frequency:\n    tot_val += key*frequency[key]\nfinal_freq = {}\nfor key in frequency:\n    final_freq[key] = key*frequency[key]/tot_val\n\n\n# printing the frequency\nprint(final_freq)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:53.009046Z","iopub.execute_input":"2023-09-12T13:17:53.009377Z","iopub.status.idle":"2023-09-12T13:17:53.020225Z","shell.execute_reply.started":"2023-09-12T13:17:53.009349Z","shell.execute_reply":"2023-09-12T13:17:53.019167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating pairs of images from the training dataset","metadata":{}},{"cell_type":"code","source":"random.seed(42)\nnewX1 = []\nnewX2 = []\nnewY = []\nfor i in range(len(transformed_train_filepaths)):\n\n    for j in range(0, len(transformed_train_filepaths[i])):\n        for k in range(0, j):\n            newX1.append(transformed_train_filepaths[i][k])\n            newX2.append(transformed_train_filepaths[i][j])\n            newY.append(0)\n\n    \n    for u in range(0, len(transformed_train_filepaths[i])):\n        step = 1\n        step = round((final_freq[len(transformed_train_filepaths[i])]*4096)/frequency[len(transformed_train_filepaths[i])])\n        no_of_iter = round(step/len(transformed_train_filepaths[i]))\n        for l in range(0, no_of_iter):\n            numbers = list(range(0, i)) + list(range(i+1, 49))\n            r = random.choice(numbers)\n            g = random.randint(0, len(transformed_train_filepaths[r]) - 1)\n            newX1.append(transformed_train_filepaths[i][u])\n            newX2.append(transformed_train_filepaths[r][g])\n            newY.append(1)\nfor i in range(0, 214):\n        \n    numbers = list(range(0,i%49)) + list(range(i%49 + 1,49))\n    r = random.choice(numbers)\n    uu = random.randint(0,len(transformed_train_filepaths[i%49])-1)\n    g = random.randint(0,len(transformed_train_filepaths[r])-1)\n    newX1.append(transformed_train_filepaths[i%49][uu])\n    newX2.append(transformed_train_filepaths[r][g])\n    newY.append(1)\nprint(len(newY))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:53.021411Z","iopub.execute_input":"2023-09-12T13:17:53.023016Z","iopub.status.idle":"2023-09-12T13:17:53.070339Z","shell.execute_reply.started":"2023-09-12T13:17:53.022985Z","shell.execute_reply":"2023-09-12T13:17:53.069147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c = list(zip(newX1, newX2, newY))\n\nrandom.shuffle(c)\n\na, b, y = zip(*c)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:53.071531Z","iopub.execute_input":"2023-09-12T13:17:53.071829Z","iopub.status.idle":"2023-09-12T13:17:53.088807Z","shell.execute_reply.started":"2023-09-12T13:17:53.071803Z","shell.execute_reply":"2023-09-12T13:17:53.087677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = []\nfor i in range(0, len(transformed_train_filepaths)):\n    train_images.extend(transformed_train_filepaths[i])\nprint(len(train_images))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:53.090653Z","iopub.execute_input":"2023-09-12T13:17:53.091082Z","iopub.status.idle":"2023-09-12T13:17:53.102204Z","shell.execute_reply.started":"2023-09-12T13:17:53.091044Z","shell.execute_reply":"2023-09-12T13:17:53.101159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images=np.array(train_images)\ntrain_images=train_images.astype('float32')/np.max(train_images)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:53.103420Z","iopub.execute_input":"2023-09-12T13:17:53.103817Z","iopub.status.idle":"2023-09-12T13:17:54.350594Z","shell.execute_reply.started":"2023-09-12T13:17:53.103775Z","shell.execute_reply":"2023-09-12T13:17:54.349314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Auto-Encoder Architecture\n","metadata":{}},{"cell_type":"code","source":"def autoencoder(img_size, code_size):\n    encoder = Sequential()\n    encoder.add(InputLayer(img_size))\n    encoder.add(Conv2D(filters=4, kernel_size=(3, 3),strides=(2,2), activation='relu', padding='same'))\n    encoder.add(MaxPooling2D((2, 2), padding='valid'))\n    encoder.add(Conv2D(filters=16, kernel_size=(3, 3),strides=(2,2), activation='relu', padding='same'))\n    encoder.add(Conv2D(filters=16, kernel_size=(3, 3),strides=(2,2), activation='relu', padding='same'))\n    encoder.add(MaxPooling2D((2, 2), padding='valid'))\n    encoder.add(Conv2D(filters=32, kernel_size=(3, 3),strides=(2,2), activation='relu', padding='same'))\n    encoder.add(Conv2D(filters=64, kernel_size=(3, 3),strides=(2,2), activation='relu', padding='same'))\n    encoder.add(Conv2D(filters=128, kernel_size=(3, 3),strides=(2,2), activation='relu', padding='same'))\n    encoder.add(MaxPooling2D((2, 2), padding='valid'))\n    encoder.add(Flatten())\n    encoder.add(Dense(units=code_size))\n    \n    decoder = Sequential()\n    decoder.add(InputLayer((code_size,)))\n    decoder.add(Dense(units=1 * 1 * 1024))\n    decoder.add(Reshape((1, 1, 1024)))\n    decoder.add(Conv2DTranspose(128, (3, 3), strides=(2, 2), activation='relu', padding='same'))\n    decoder.add(Conv2DTranspose(64, (3, 3), strides=(2, 2), activation='relu', padding='same'))\n    decoder.add(Conv2DTranspose(32, (3, 3), strides=(2, 2), activation='relu', padding='same'))\n    decoder.add(UpSampling2D((2, 2)))\n    decoder.add(Conv2DTranspose(16, (3, 3), strides=(2, 2), activation='relu', padding='same'))\n    decoder.add(Conv2DTranspose(16, (3, 3), strides=(2, 2), activation='relu', padding='same'))\n    decoder.add(UpSampling2D((2, 2)))\n    decoder.add(Conv2DTranspose(4, (3, 3), strides=(2, 2), activation='relu', padding='same'))\n    decoder.add(Conv2DTranspose(3, (3, 3), strides=(2, 2), activation='sigmoid', padding='same'))\n    \n    return encoder, decoder","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:54.352443Z","iopub.execute_input":"2023-09-12T13:17:54.353066Z","iopub.status.idle":"2023-09-12T13:17:54.369751Z","shell.execute_reply.started":"2023-09-12T13:17:54.353034Z","shell.execute_reply":"2023-09-12T13:17:54.368814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder, decoder = autoencoder((512, 512, 3), 512)\nencoder.summary()\ndecoder.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:54.371118Z","iopub.execute_input":"2023-09-12T13:17:54.372293Z","iopub.status.idle":"2023-09-12T13:17:55.157985Z","shell.execute_reply.started":"2023-09-12T13:17:54.372249Z","shell.execute_reply":"2023-09-12T13:17:55.156872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp = Input((512, 512, 3))\nautoencoder = Model(inputs=inp, outputs=decoder(encoder(inp)))\nautoencoder.compile(optimizer='adam', loss='mse')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:55.159471Z","iopub.execute_input":"2023-09-12T13:17:55.159801Z","iopub.status.idle":"2023-09-12T13:17:55.328549Z","shell.execute_reply.started":"2023-09-12T13:17:55.159773Z","shell.execute_reply":"2023-09-12T13:17:55.327313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting the AutoEncoder","metadata":{}},{"cell_type":"code","source":"epochs = 150\nbatch_size = 16\n\nhistory = autoencoder.fit(train_images, train_images, epochs=epochs, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:17:55.335690Z","iopub.execute_input":"2023-09-12T13:17:55.336076Z","iopub.status.idle":"2023-09-12T13:18:32.694378Z","shell.execute_reply.started":"2023-09-12T13:17:55.336044Z","shell.execute_reply":"2023-09-12T13:18:32.693328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extracting features using the encoder block of the AutoEncoder","metadata":{}},{"cell_type":"code","source":"output1 = []\noutput2 = []\nfor i in range(1, len(a)//256 + 1):\n    a1 = a[(i-1)*256 : i*256]\n    b1 = b[(i-1)*256 : i*256]\n    output1.extend(encoder.predict(np.array(a1)/255))\n    output2.extend(encoder.predict(np.array(b1)/255))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:18:32.695795Z","iopub.execute_input":"2023-09-12T13:18:32.696320Z","iopub.status.idle":"2023-09-12T13:22:45.251572Z","shell.execute_reply.started":"2023-09-12T13:18:32.696291Z","shell.execute_reply":"2023-09-12T13:22:45.250263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexes1=[i for i,x in enumerate(y) if x == 1]\nindexes0=[i for i,x in enumerate(y) if x == 0]","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:22:45.254008Z","iopub.execute_input":"2023-09-12T13:22:45.254423Z","iopub.status.idle":"2023-09-12T13:22:45.262773Z","shell.execute_reply.started":"2023-09-12T13:22:45.254388Z","shell.execute_reply":"2023-09-12T13:22:45.261922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features Subtraction\n\n- Getting absolute value of the difference between feature vectors of two images\n- Plotting graphs for positive and negative image pairs","metadata":{}},{"cell_type":"code","source":"arr=[]\nsu=[]\nfor s in range(len(output1)):\n    oo = np.abs(np.subtract(np.array(output1[s]),np.array(output2[s])))\n    arr.append(oo)\n    su.append(oo.sum())\n    \na = np.array(su)\nsu1=list(a[indexes1])\nsu0=list(a[indexes0])\n\nfig, axs = plt.subplots(1, 2)\nfig.set_size_inches(18, 4)\nfig.suptitle(\"Sum differences\")\naxs[0].plot(list(range(4096)),su1, list(range(4096)),su0)\naxs[0].legend([\"different people\", \"same person\"])\n#axs[0].title(\"Euclidean distance\")\naxs[1].plot(list(range(8192)),su)\naxs[1].legend([\"overall variation\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:22:45.263994Z","iopub.execute_input":"2023-09-12T13:22:45.264816Z","iopub.status.idle":"2023-09-12T13:22:46.469724Z","shell.execute_reply.started":"2023-09-12T13:22:45.264785Z","shell.execute_reply":"2023-09-12T13:22:46.468913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using a simple neural network for classifying the image pairs as those of the same person or those of different peopleÂ¶","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras.backend as K\nimport tensorflow\n\n\ndef distance(vecs):\n    x, y = vecs\n    x = K.l2_normalize(x, axis=-1)\n    y = K.l2_normalize(y, axis=-1)\n    \n    return K.abs(x-y)\n\n\nfeaturesA=Input(512, )\nfeaturesB=Input(512, )\ndistance= Lambda(distance)([featuresA,featuresB])\n\nx= Dense(96, activation=\"relu\")(distance)\nx= Dropout(0.3)(x)\nx= Dense(64)(x)\noutputs = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=[featuresA, featuresB],outputs=outputs)\nmodel.compile(loss='binary_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:22:46.470845Z","iopub.execute_input":"2023-09-12T13:22:46.471350Z","iopub.status.idle":"2023-09-12T13:22:46.588594Z","shell.execute_reply.started":"2023-09-12T13:22:46.471321Z","shell.execute_reply":"2023-09-12T13:22:46.587509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting the classifier and checking the validation accuracy","metadata":{}},{"cell_type":"code","source":"history=model.fit([np.array(output1)[:6144], np.array(output2)[:6144]],np.array(y)[:6144],validation_data=([np.array(output1)[6144:], np.array(output2)[6144:]],np.array(y)[6144:]), epochs=10, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:22:46.589991Z","iopub.execute_input":"2023-09-12T13:22:46.590325Z","iopub.status.idle":"2023-09-12T13:22:57.770603Z","shell.execute_reply.started":"2023-09-12T13:22:46.590296Z","shell.execute_reply":"2023-09-12T13:22:57.769361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2)\nfig.set_size_inches(18, 4)\nfig.suptitle(\"Overfitting analysis\")\naxs[0].plot(list(range(1,11)), history.history['val_accuracy'], list(range(1,11)), history.history['accuracy'])\n\naxs[0].title.set_text(\"Accuracy\")\naxs[0].legend([\"validation accuracy\", \"training accuracy\"])\naxs[1].plot(list(range(1,11)), history.history['val_loss'], list(range(1,11)), history.history['loss'])\naxs[1].title.set_text('Loss')\naxs[1].legend([\"validation loss\", \"trainig loss\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:22:57.773043Z","iopub.execute_input":"2023-09-12T13:22:57.773551Z","iopub.status.idle":"2023-09-12T13:22:58.395156Z","shell.execute_reply.started":"2023-09-12T13:22:57.773517Z","shell.execute_reply":"2023-09-12T13:22:58.394010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"transformed_test_filepaths = []\nfor filename in test_filepaths:\n    temp = []\n    for imagename in filename:\n        img = cv2.imread(imagename)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n\n        faces = face_cascade.detectMultiScale(img, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50))\n        face_image = img\n        if(len(faces) > 0):\n            xf, yf, wf, hf = faces[0]\n            face_image = img[yf:yf+hf, xf:xf+wf]\n        face_image = cv2.resize(face_image, (512, 512))\n        \n\n        temp.append(face_image)\n    transformed_test_filepaths.append(temp)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:22:58.396823Z","iopub.execute_input":"2023-09-12T13:22:58.397191Z","iopub.status.idle":"2023-09-12T13:23:08.710148Z","shell.execute_reply.started":"2023-09-12T13:22:58.397161Z","shell.execute_reply":"2023-09-12T13:23:08.709025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum = 0\nlis = []\nfrequency = {}\nfor i in range(0, len(transformed_test_filepaths)):\n    sum += ((len(transformed_test_filepaths[i]))*(len(transformed_test_filepaths[i]) - 1))/2\n    lis.append(len(transformed_test_filepaths[i]))\n\nfor item in lis:\n   # checking the element in dictionary\n   if item in frequency:\n      # incrementing the count\n      frequency[item] += 1\n   else:\n      # initializing the count\n      frequency[item] = 1\ntot_val = 0\nfor key in frequency:\n    tot_val += key*frequency[key]\nfinal_freq = {}\nfor key in frequency:\n    final_freq[key] = key*frequency[key]/tot_val\n\n\n# printing the frequency\nprint(final_freq)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:23:08.711376Z","iopub.execute_input":"2023-09-12T13:23:08.711699Z","iopub.status.idle":"2023-09-12T13:23:08.720867Z","shell.execute_reply.started":"2023-09-12T13:23:08.711653Z","shell.execute_reply":"2023-09-12T13:23:08.719724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating pairs of images from the test dataset for evaluating the entire pipeline","metadata":{}},{"cell_type":"code","source":"random.seed(42)\nnewX1_test = []\nnewX2_test = []\nnewY_test = []\nfor i in range(len(transformed_test_filepaths)):\n    for j in range(0, len(transformed_test_filepaths[i])):\n        for k in range(0, j):\n            newX1_test.append(transformed_test_filepaths[i][k])\n            newX2_test.append(transformed_test_filepaths[i][j])\n            newY_test.append(0)\n    \n    for u in range(0, len(transformed_test_filepaths[i])):\n        step = 1\n        step = round((final_freq[len(transformed_test_filepaths[i])]*264)/frequency[len(transformed_test_filepaths[i])])\n        no_of_iter = round(step/len(transformed_test_filepaths[i]))\n        for l in range(0, 1):\n            numbers = list(range(0, i)) + list(range(i+1, 49))\n            r = random.choice(numbers)\n            g = random.randint(0, len(transformed_test_filepaths[r]) - 1)\n            newX1_test.append(transformed_test_filepaths[i][u])\n            newX2_test.append(transformed_test_filepaths[r][g])\n            newY_test.append(1)\nfor i in range(0, 79):\n        \n    numbers = list(range(0,i%49)) + list(range(i%49 + 1,49))\n    r = random.choice(numbers)\n    uu = random.randint(0,len(transformed_test_filepaths[i%49])-1)\n    g = random.randint(0,len(transformed_test_filepaths[r])-1)\n    newX1_test.append(transformed_test_filepaths[i%49][uu])\n    newX2_test.append(transformed_test_filepaths[r][g])\n    newY_test.append(1)\nprint(len(newY_test))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:23:08.722306Z","iopub.execute_input":"2023-09-12T13:23:08.722595Z","iopub.status.idle":"2023-09-12T13:23:08.740699Z","shell.execute_reply.started":"2023-09-12T13:23:08.722569Z","shell.execute_reply":"2023-09-12T13:23:08.739636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c_test = list(zip(newX1_test, newX2_test, newY_test))\n\nrandom.shuffle(c_test)\n\na_test, b_test, y_test = zip(*c_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:23:08.743649Z","iopub.execute_input":"2023-09-12T13:23:08.744005Z","iopub.status.idle":"2023-09-12T13:23:08.756395Z","shell.execute_reply.started":"2023-09-12T13:23:08.743975Z","shell.execute_reply":"2023-09-12T13:23:08.755301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(a_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:23:08.757969Z","iopub.execute_input":"2023-09-12T13:23:08.758279Z","iopub.status.idle":"2023-09-12T13:23:08.769704Z","shell.execute_reply.started":"2023-09-12T13:23:08.758253Z","shell.execute_reply":"2023-09-12T13:23:08.768815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extracting features using the encoder block of the AutoEncoder","metadata":{}},{"cell_type":"code","source":"output1_test = []\noutput2_test = []\nfor i in range(1, len(a_test)//264 + 1):\n    a1_test = a_test[(i-1)*264 : i*264]\n    b1_test = b_test[(i-1)*264 : i*264]\n    output1_test.extend(encoder.predict(np.array(a1_test)/255))\n    output2_test.extend(encoder.predict(np.array(b1_test)/255))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:23:08.771141Z","iopub.execute_input":"2023-09-12T13:23:08.771435Z","iopub.status.idle":"2023-09-12T13:23:25.140764Z","shell.execute_reply.started":"2023-09-12T13:23:08.771409Z","shell.execute_reply":"2023-09-12T13:23:25.139901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexes1_test=[i for i,x in enumerate(y_test) if x == 1]\nindexes0_test=[i for i,x in enumerate(y_test) if x == 0]","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:23:25.141952Z","iopub.execute_input":"2023-09-12T13:23:25.142825Z","iopub.status.idle":"2023-09-12T13:23:25.148594Z","shell.execute_reply.started":"2023-09-12T13:23:25.142793Z","shell.execute_reply":"2023-09-12T13:23:25.147358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using the classifier for final training and evaluation","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras.backend as K\nimport tensorflow\n\n\ndef distance(vecs):\n    x, y = vecs\n    x = K.l2_normalize(x, axis=-1)\n    y = K.l2_normalize(y, axis=-1)\n    \n    return K.abs(x-y)\n\n\nfeaturesA=Input(512, )\nfeaturesB=Input(512, )\ndistance= Lambda(distance)([featuresA,featuresB])\n\nx= Dense(96, activation=\"relu\")(distance)\nx= Dropout(0.3)(x)\nx= Dense(64)(x)\noutputs = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=[featuresA, featuresB],outputs=outputs)\nmodel.compile(loss='binary_crossentropy', optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:23:25.150099Z","iopub.execute_input":"2023-09-12T13:23:25.150447Z","iopub.status.idle":"2023-09-12T13:23:25.252357Z","shell.execute_reply.started":"2023-09-12T13:23:25.150417Z","shell.execute_reply":"2023-09-12T13:23:25.251186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(output1))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:23:25.254129Z","iopub.execute_input":"2023-09-12T13:23:25.254690Z","iopub.status.idle":"2023-09-12T13:23:25.261273Z","shell.execute_reply.started":"2023-09-12T13:23:25.254596Z","shell.execute_reply":"2023-09-12T13:23:25.260031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting the classifier on the entire training data","metadata":{}},{"cell_type":"code","source":"history=model.fit([np.array(output1)[:8192], np.array(output2)[:8192]],np.array(y)[:8192], epochs=15, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:23:25.263018Z","iopub.execute_input":"2023-09-12T13:23:25.263443Z","iopub.status.idle":"2023-09-12T13:23:34.355338Z","shell.execute_reply.started":"2023-09-12T13:23:25.263403Z","shell.execute_reply":"2023-09-12T13:23:34.354240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Obtaining final test results on the entire test data","metadata":{}},{"cell_type":"code","source":"test_res = model.evaluate([np.array(output1_test)[:528], np.array(output2_test)[:528]],np.array(y_test)[:528], batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T13:23:34.356576Z","iopub.execute_input":"2023-09-12T13:23:34.356930Z","iopub.status.idle":"2023-09-12T13:23:34.627285Z","shell.execute_reply.started":"2023-09-12T13:23:34.356891Z","shell.execute_reply":"2023-09-12T13:23:34.626196Z"},"trusted":true},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}